{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2aff3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from wordcloud import STOPWORDS\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ccecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/colearninglounge/nlp-data-preprocessing-and-cleaning\n",
    "\n",
    "def remove_hashtags(data):\n",
    "    clean_data = re.sub(r\"#[A-Za-z0-9_]+\", \"\", data)\n",
    "    clean_data = re.sub(r\"\\$[A-Za-z0-9_]+\", \"\", clean_data)\n",
    "    return clean_data\n",
    "\n",
    "def remove_hashsymbol(data):\n",
    "    clean_data = re.sub(r\"#\", \"\", data)\n",
    "    return clean_data\n",
    "\n",
    "def remove_punctuations(data):\n",
    "    punct_tag=re.compile(r'[^\\w\\s]')\n",
    "    data=punct_tag.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "def remove_comma(data):\n",
    "    punct_tag=re.compile(r'[^\\w\\s]')\n",
    "    data=punct_tag.sub(r' ',data)\n",
    "    return data\n",
    "\n",
    "def remove_html(data):\n",
    "    data = re.sub(r\"https?://[^\\s]+\", \"\", data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def remove_url(data):\n",
    "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    data=url_clean.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "def remove_html(data):\n",
    "    data = re.sub(r\"https?://[^\\s]+\", \"\", data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def remove_url(data):\n",
    "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
    "    data=url_clean.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "def remove_stopwords(df, column):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.add(\"see\")\n",
    "    \n",
    "    def remove_stopwords_from_text(text):\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_tokens = [token for token in word_tokens if not token.lower() in stop_words]\n",
    "        return \" \".join(filtered_tokens)\n",
    "    \n",
    "    df[column] = df[column].apply(remove_stopwords_from_text)\n",
    "    return df\n",
    "\n",
    "def clean_text_linkedIn(text):\n",
    "    text = clean(text ,no_urls = True ,no_emoji=True, no_emails = True, lower = False, no_punct = False, no_currency_symbols=True, no_phone_numbers=True, replace_with_url=\"\", replace_with_currency_symbol=\"\", replace_with_email=\"\", replace_with_phone_number=\"\")\n",
    "    return text\n",
    "\n",
    "def convert_lowercase(text):\n",
    "    return clean(text, lower = True)\n",
    "\n",
    "# df2_cleaned_txt[\"text\"] = df2_cleaned_txt[\"text\"].apply(remove_hashtags)\n",
    "# df2_cleaned_txt[\"text\"] = df2_cleaned_txt[\"text\"].apply(clean_text_linkedin)\n",
    "# df2_cleaned_txt = remove_stopwords(df2_cleaned_txt, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e3bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sent = pd.read_csv(\"/Users/sreevaatsav/Desktop/Data_merge/deduped_linkedin_feb25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae61f176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28519, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8afe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sent[\"text\"] = df2_sent[\"text\"].apply(remove_hashtags)\n",
    "df2_sent[\"text\"] = df2_sent[\"text\"].apply(remove_comma)\n",
    "df2_sent[\"text\"] = df2_sent[\"text\"].apply(remove_html)\n",
    "df2_sent[\"text\"] = df2_sent[\"text\"].apply(remove_punctuations)\n",
    "df2_sent[\"text\"] = df2_sent[\"text\"].apply(clean_text_linkedIn)\n",
    "df2_sent[\"text\"] = df2_sent[\"text\"].apply(convert_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f2155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotions(df):\n",
    "    \n",
    "    emotions_arr = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        try:\n",
    "            classes = emotion_clf(df.iloc[i][\"text\"])[0]\n",
    "            temp_arr = []\n",
    "            for cls_dic in classes:\n",
    "    #             print(cls_dic)\n",
    "                temp_arr.append(cls_dic[\"score\"])\n",
    "    #         print()\n",
    "            emotions_arr.append(temp_arr)\n",
    "            \n",
    "        except:\n",
    "            emotions_arr.append([None]*7)\n",
    "\n",
    "\n",
    "    return emotions_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d380cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6428100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "emotion_clf = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a1618a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.24813131491343  mins\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "s = time.time()\n",
    "emotions_arr = get_emotions(df2_sent)\n",
    "e = time.time()\n",
    "print((e-s)/60, \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d313706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions_arr\n",
    "df2_emotions = df2_sent.copy()\n",
    "sent_df_obtained = pd.DataFrame(emotions_arr, columns=['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'])\n",
    "df2_emotions = pd.concat([df2_emotions, sent_df_obtained], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50fc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df2_emotions[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f46387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets_merged_feb25.csv',\n",
       " 'locations_tweets.csv',\n",
       " '.DS_Store',\n",
       " 'requirements.txt',\n",
       " 'Untitled.ipynb',\n",
       " '25.html',\n",
       " 'recession_tweets.csv',\n",
       " 'anger_posts',\n",
       " 'cities1000.zip',\n",
       " 'Temp.ipynb',\n",
       " 'emotions_1_linkedin.csv',\n",
       " '.gitignore',\n",
       " 'temp.py',\n",
       " 'anger_posts.html',\n",
       " 'world-countries.json',\n",
       " '.python-version',\n",
       " '.ipynb_checkpoints',\n",
       " 'Analysis_vats.ipynb',\n",
       " 'cities1000.txt',\n",
       " 'world.json',\n",
       " 'us-states.json',\n",
       " '.git',\n",
       " 'linkedin_merged.csv',\n",
       " 'py-googletrans',\n",
       " 'Analysis.ipynb',\n",
       " '25']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6423f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_emotions.to_csv(\"emotions_2_linkedin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2660f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e840f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c027f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bef13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
