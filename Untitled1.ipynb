{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f918eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pandasql import sqldf\n",
    "import random\n",
    "from datetime import date\n",
    "import datetime\n",
    "import scipy\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import STOPWORDS\n",
    "from cleantext import clean\n",
    "from unidecode import unidecode\n",
    "import duckdb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2465348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "# # Create a regression dataset\n",
    "# X_reg, y_reg = make_regression(n_features=4, random_state=0)\n",
    "# X_reg = torch.tensor(X_reg).float()\n",
    "# y_reg = torch.tensor(y_reg).float().unsqueeze(1)\n",
    "\n",
    "# # Create a classification dataset\n",
    "# X_class, y_class = make_classification(n_features=4, random_state=0)\n",
    "# X_class = torch.tensor(X_class).float()\n",
    "# y_class = torch.tensor(y_class).long()\n",
    "\n",
    "# # Define a neural network model\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 10)\n",
    "#         self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize the model and define the loss function and optimizer\n",
    "# model = Net()\n",
    "# criterion_reg = nn.MSELoss()\n",
    "# criterion_class = nn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "\n",
    "# reg_error = []\n",
    "\n",
    "# # Train the model for regression\n",
    "# for epoch in range(5000):\n",
    "#     # Forward pass\n",
    "#     y_pred = model(X_reg)\n",
    "#     loss = criterion_reg(y_pred, y_reg)\n",
    "\n",
    "#     # Backward pass and optimization\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     reg_error.append(loss.item())\n",
    "\n",
    "# # Train the model for classification\n",
    "# # for epoch in range(100):\n",
    "# #     # Forward pass\n",
    "# #     y_pred = model(X_class)\n",
    "# #     loss = criterion_class(y_pred, y_class)\n",
    "\n",
    "# #     # Backward pass and optimization\n",
    "# #     optimizer.zero_grad()\n",
    "# #     loss.backward()\n",
    "# #     optimizer.step()\n",
    "# plt.plot(reg_error, label='Regression')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Training Error')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b21ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from sklearn.datasets import make_regression, make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a regression dataset and split it into training and validation sets\n",
    "# X_reg, y_reg = make_regression(n_features=4, random_state=0)\n",
    "# X_reg_train, X_reg_val, y_reg_train, y_reg_val = train_test_split(X_reg, y_reg, test_size=0.2, random_state=0)\n",
    "# X_reg_train = torch.tensor(X_reg_train).float()\n",
    "# y_reg_train = torch.tensor(y_reg_train).float().unsqueeze(1)\n",
    "# X_reg_val = torch.tensor(X_reg_val).float()\n",
    "# y_reg_val = torch.tensor(y_reg_val).float().unsqueeze(1)\n",
    "\n",
    "# # Create a classification dataset and split it into training and validation sets\n",
    "# X_class, y_class = make_classification(n_features=4, random_state=0)\n",
    "# X_class_train, X_class_val, y_class_train, y_class_val = train_test_split(X_class, y_class, test_size=0.2, random_state=0)\n",
    "# X_class_train = torch.tensor(X_class_train).float()\n",
    "# y_class_train = torch.tensor(y_class_train).long()\n",
    "# X_class_val = torch.tensor(X_class_val).float()\n",
    "# y_class_val = torch.tensor(y_class_val).long()\n",
    "\n",
    "# # Define a neural network model\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 10)\n",
    "#         self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize the model and define the loss function and optimizer\n",
    "# model = Net()\n",
    "# criterion_reg = nn.MSELoss()\n",
    "# criterion_class = nn.CrossEntropyLoss()\n",
    "\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "\n",
    "\n",
    "# # Track the training and validation error for regression\n",
    "# reg_error_train = []\n",
    "# reg_error_val = []\n",
    "\n",
    "# # Train the model for regression\n",
    "# for epoch in range(5000):\n",
    "#     # Forward pass on the training data\n",
    "#     y_pred_train = model(X_reg_train)\n",
    "#     loss_train = criterion_reg(y_pred_train, y_reg_train)\n",
    "\n",
    "#     # Forward pass on the validation data\n",
    "#     y_pred_val = model(X_reg_val)\n",
    "#     loss_val = criterion_reg(y_pred_val, y_reg_val)\n",
    "\n",
    "#     # Backward pass and optimization\n",
    "#     optimizer.zero_grad()\n",
    "#     loss_train.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Track the training and validation error\n",
    "#     reg_error_train.append(loss_train.item())\n",
    "#     reg_error_val.append(loss_val.item())\n",
    "\n",
    "# # # Track the training and validation error for classification\n",
    "# # class_error_train = []\n",
    "# # class_error_val = []\n",
    "\n",
    "# # # Train the model for classification\n",
    "# # for epoch in range(100):\n",
    "# #     # Forward pass on the training data\n",
    "# #     y_pred_train = model(X_class_train)\n",
    "# #     loss_train = criterion_class(y_pred_train, y_class_train)\n",
    "\n",
    "# #     # Forward pass on the validation data\n",
    "# #     y_pred_val = model(X_class_val)\n",
    "# #     loss_val = criterion_class(y_pred_val, y_class_val)\n",
    "\n",
    "# #     # Backward pass and optimization\n",
    "# #     optimizer.zero_grad()\n",
    "# #     loss_train.backward()\n",
    "# #     optimizer.step()\n",
    "\n",
    "# #     # Track the training and validation error\n",
    "# #     class_error_train.append(loss_train.item())\n",
    "# #     class_error_val.append(loss_val.item())\n",
    "\n",
    "# # Plot the training and validation error for regression and classification\n",
    "# plt.plot(reg_error_train, label='Regression Training')\n",
    "# plt.plot(reg_error_val, label='Regression Validation')\n",
    "# # plt.plot(class_error_train, label='Classification Training')\n",
    "# # plt.plot(class_error_val, label='Classification Validation')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Error')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492409d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc038d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Assuming you have your data and labels in numpy arrays `data` and `labels`\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mdata\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     38\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(labels)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     39\u001b[0m dataset \u001b[38;5;241m=\u001b[39m MotionDataset(data, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MotionDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class MotionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MotionModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3 # ax, ay, az\n",
    "hidden_size = 128\n",
    "num_classes = 4 # number of motion types\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load data\n",
    "# Assuming you have your data and labels in numpy arrays `data` and `labels`\n",
    "data = torch.from_numpy(data).float()\n",
    "labels = torch.from_numpy(labels).long()\n",
    "dataset = MotionDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model\n",
    "model = MotionModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_batch, labels_batch) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(data_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50c03330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpcircle = '/Users/sreevaatsav/Downloads/Data_challenge/circ_accel_data.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fpcircle = '/Users/sreevaatsav/Downloads/Data_challenge/hop_accel_data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af76afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_circle = pd.read_csv(fpcircle, delimiter=\" \", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d8f123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_circle.columns = [\"ax\", \"ay\",\"az\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f09059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_circle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3561a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df_circle = df_circle.iloc[0:5000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "908c3682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250000.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20000000)*0.0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1767b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_circle = df_circle.iloc[5000000:6250000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "322a4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000000+1250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc2d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa6bc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_circle.to_csv('/Users/sreevaatsav/Downloads/Data_challenge/circ_accel_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488f312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
